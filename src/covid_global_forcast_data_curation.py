# -*- coding: utf-8 -*-
"""covid-global-forcast-data-curation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13yyd8Y8Nb3iGddErHxqVJn7UOjZA-TMA
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import requests 
import os
import pandas_profiling
pd.set_option("display.max_columns",100)
# https://population.un.org/wpp/Download/Standard/Population/


# Function to Download csv files from open source
def downloadFiles(listOfFiles):
    base_url="https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/"
    for fileName in listOfFiles:
        url=base_url+fileName
        r= requests.get(url)
        if r.status_code== 200:
            with open(fileName,'wb') as file:
                file.write(r.content)
def createBaseDS():
    
    confirmed_cases="time_series_covid19_confirmed_global.csv"
    recovered="time_series_covid19_recovered_global.csv"
    dealths="time_series_covid19_deaths_global.csv"
    # extract dates


    files=[confirmed_cases,recovered,dealths]
    downloadFiles(files)
    csv_names=["# Confirmed Cases","# Recovered Cases","# Fatalities"]
    # creating a dictionary of our dataframes that can be accesed by their name listed above
    dfs={x:pd.read_csv(y,index_col=False) for (x,y) in zip(csv_names,files)}

    dates = dfs[csv_names[0]].columns[4:]

    # melt dataframes into longer format
    # ==================================

    for value, df in dfs.items():
        dfs[value]= df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], 
                                value_vars=dates, var_name='Date', value_name=value)
        print(f"{value} has {dfs[value].shape}")


  
    # merge dataframes
    # ================

    full_table = pd.merge(left=dfs[csv_names[0]], right=dfs[csv_names[1]], how='left',
                          on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long'])
    full_table = pd.merge(left=full_table, right=dfs[csv_names[2]], how='left',
                          on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long'])

    full_table.head()



    who_region = {}

    # African Region AFRO
    afro = "Algeria, Angola, Cabo Verde, Eswatini, Sao Tome and Principe, Benin, South Sudan, Western Sahara, Congo (Brazzaville), Congo (Kinshasa), Cote d'Ivoire, Botswana, Burkina Faso, Burundi, Cameroon, Cape Verde, Central African Republic, Chad, Comoros, Ivory Coast, Democratic Republic of the Congo, Equatorial Guinea, Eritrea, Ethiopia, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, Kenya, Lesotho, Liberia, Madagascar, Malawi, Mali, Mauritania, Mauritius, Mozambique, Namibia, Niger, Nigeria, Republic of the Congo, Rwanda, São Tomé and Príncipe, Senegal, Seychelles, Sierra Leone, Somalia, South Africa, Swaziland, Togo, Uganda, Tanzania, Zambia, Zimbabwe"
    afro = [i.strip() for i in afro.split(',')]
    for i in afro:
        who_region[i] = 'Africa'

    # Region of the Americas PAHO
    paho = 'Antigua and Barbuda, Argentina, Bahamas, Barbados, Belize, Bolivia, Brazil, Canada, Chile, Colombia, Costa Rica, Cuba, Dominica, Dominican Republic, Ecuador, El Salvador, Grenada, Guatemala, Guyana, Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Suriname, Trinidad and Tobago, United States, US, Uruguay, Venezuela'
    paho = [i.strip() for i in paho.split(',')]
    for i in paho:
        who_region[i] = 'Americas'

    # South-East Asia Region SEARO
    searo = 'Bangladesh, Bhutan, North Korea, India, Indonesia, Maldives, Myanmar, Burma, Nepal, Sri Lanka, Thailand, Timor-Leste'
    searo = [i.strip() for i in searo.split(',')]
    for i in searo:
        who_region[i] = 'South-East Asia'

    # European Region EURO
    euro = 'Albania, Andorra, Greenland, Kosovo, Holy See, Liechtenstein, Armenia, Czechia, Austria, Azerbaijan, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Israel, Italy, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Luxembourg, Malta, Monaco, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Moldova, Romania, Russia, San Marino, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Tajikistan, Turkey, Turkmenistan, Ukraine, United Kingdom, Uzbekistan'
    euro = [i.strip() for i in euro.split(',')]
    for i in euro:
        who_region[i] = 'Europe'


    # Eastern Mediterranean Region EMRO
    emro = 'Afghanistan, Bahrain, Djibouti, Egypt, Iran, Iraq, Jordan, Kuwait, Lebanon, Libya, Morocco, Oman, Pakistan, Palestine, West Bank and Gaza, Qatar, Saudi Arabia, Somalia, Sudan, Syria, Tunisia, United Arab Emirates, Yemen'
    emro = [i.strip() for i in emro.split(',')]
    for i in emro:
        who_region[i] = 'Eastern Mediterranean'

    # Western Pacific Region WPRO
    wpro = 'Australia, Brunei, Cambodia, China, Cook Islands, Fiji, Japan, Kiribati, Laos, Malaysia, Marshall Islands, Micronesia, Mongolia, Nauru, New Zealand, Niue, Palau, Papua New Guinea, Philippines, South Korea, Samoa, Singapore, Solomon Islands, Taiwan, Taiwan*, Tonga, Tuvalu, Vanuatu, Vietnam'
    wpro = [i.strip() for i in wpro.split(',')]
    for i in wpro:
        who_region[i] = 'Western Pacific'

    """
    # add 'WHO Region' column
    full_table['WHO Region'] = full_table['Country/Region'].map(who_region)

    # find missing values
    full_table[full_table['WHO Region'].isna()]['Country/Region'].unique()
    """
    full_table["Date"]=pd.to_datetime(full_table["Date"],infer_datetime_format=True)  
    filt= full_table["Country/Region"] == "Canada"
    return full_table[filt]    
    
def createPopulationDS():
    POPULATION_URL="https://www.worldometers.info/world-population/population-by-country/"
    fileName="population.html"
    r= requests.get(POPULATION_URL)
    if r.status_code== 200:
        with open(fileName,'wb') as file:
            file.write(r.content)
    ds=pd.read_html(fileName)[0]
    return ds
def createOwidDS():
  # https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-codebook.csv
  OWID_URL="https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"
  r= requests.get(OWID_URL)
  fileName="owid-covid-data.csv"
  if r.status_code== 200:
      with open(fileName,'wb') as file:
          file.write(r.content)
  ds=pd.read_csv(fileName,parse_dates=["date"])
  cols=['continent', 'location', 'date',  'stringency_index',
       'population', 'population_density', 'median_age', 'aged_65_older',
       'aged_70_older', 'gdp_per_capita', 'extreme_poverty',
       'cardiovasc_death_rate', 'diabetes_prevalence', 'female_smokers',
       'male_smokers', 'handwashing_facilities', 'hospital_beds_per_thousand',
       'life_expectancy']
  ds["date"]=pd.to_datetime(ds["date"],infer_datetime_format=True)  

  return ds


                
if __name__ == "__main__":
  baseDF=createBaseDS()
  popDF=createPopulationDS()
  owidDF=createOwidDS()

  dfs=[baseDF,popDF,owidDF]

for x in dfs: 
  x.info()

popDF["Country (or dependency)"]=popDF["Country (or dependency)"].replace({'Myanmar':"Burma",'Czech Republic (Czechia)':'Czechia'})
baseDF["Country/Region"]=baseDF["Country/Region"].replace({"Cote d'Ivoire" :"Côte d'Ivoire",'Korea, South':"South Korea","Taiwan*":"Taiwan","US":'United States'})

congoDF2=baseDF.groupby(by="Country/Region").get_group('Congo (Brazzaville)').copy()
congoDF1=baseDF.groupby(by="Country/Region").get_group('Congo (Kinshasa)').copy()
congoDF1.reset_index(inplace=True)
congoDF2.reset_index(inplace=True)

cols= ['confirmed_cases','deaths','recovered_cases']
for x in cols:
  congoDF1[x]+=  congoDF2[x]


congoDF1["Country/Region"].replace('Congo (Kinshasa)','Congo',inplace=True)
congoDF1.fillna(0,inplace=True)
filt=(baseDF["Country/Region"]=='Congo (Kinshasa)')| (baseDF["Country/Region"]=='Congo (Brazzaville)')
baseDF= baseDF.drop(baseDF[filt].index)
baseDF=baseDF.append(congoDF1)

unWantedLabels=["Diamond Princess", 'Kosovo', 'MS Zaandam', 'Saint Kitts and Nevis',
                'Saint Vincent and the Grenadines', 
                'Sao Tome and Principe' ,'West Bank and Gaza'
                , 'Holy See',  'MS Zaandam', 'Timor-Leste']
for label in unWantedLabels:
  filt=baseDF["Country/Region"]==label
  baseDF=baseDF.drop(labels=baseDF[filt].index)

owidDF["location"]=owidDF["location"].replace({'Cape Verde':'Cabo Verde','Macedonia':'North Macedonia','Swaziland':'Eswatini','Myanmar':'Burma','Czech Republic':'Czechia',"Cote d'Ivoire" :"Côte d'Ivoire"})

baseDFCountries=set(baseDF["Country/Region"])
popCount=set(dfs[1]["Country (or dependency)"])
owidCountries=set(dfs[2]["location"])
popDiff1= sorted(baseDFCountries.difference(popCount))
owidCount= sorted(baseDFCountries.difference(owidCountries))


print(popDiff1)
print(owidCount)

owidDates=set(owidDF['date'])
baseDFDates=set(baseDF["Date"])
diff= baseDFDates.difference(owidDates)
temp=owidDF.groupby('location').get_group('Burma')
temp.describe()

final_csv=pd.merge(left=baseDF,right=popDF,left_on="Country/Region",right_on="Country (or dependency)")
final_csv=pd.merge(left=final_csv,right=owidDF,left_on=["Country/Region","Date"],right_on=["location","date"])

unIntersted_cols=['Country (or dependency)','index','#','iso_code','location','date', 'total_cases', 'new_cases',
       'new_cases_smoothed', 'total_deaths', 'new_deaths',
       'new_deaths_smoothed', 'total_cases_per_million',
       'new_cases_per_million', 'new_cases_smoothed_per_million',
       'total_deaths_per_million',  'new_deaths_per_million','new_deaths_smoothed_per_million','total_tests',
       'total_tests_per_thousand', 'new_tests_per_thousand',
       'new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'population', 'population_density','World Share']
final_csv=final_csv.drop(columns=unIntersted_cols)
print(final_csv.columns)


final_csv.info()

groups= final_csv.groupby("Country/Region")
for name , group in groups:
  csv_names=["confirmed_cases","recovered_cases","deaths"]
  value_dict={col:v for col,v in zip(csv_names,['sum' for x in csv_names]) }
  group.groupby('Province/State').agg(value_dict)

final_csv['Province/State'].unique()

final_csv[final_csv['Country/Region'] == 'Canada']

final_csv[final_csv['Country/Region'] == 'United States']

final_csv['Country/Region'].unique()

canadaDF= createBaseDS().sort_values(by=["Province/State"])
canadaDF.info()